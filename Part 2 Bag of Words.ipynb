{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll try out the most straightforward way to represent a document as a fixed length vector\n",
    "\n",
    "* CountVectorizer: length = size of vocabulary, entry = the number of occurrence of words with the given ID/index\n",
    "* TfidfVectorizer: Term-frequency multiplied by inverse document frequency\n",
    "* HashingVectorizer: Convert document into vector using hashing function\n",
    "\n",
    "We'll also pipeline the vectorizer with two standardization methods:\n",
    "* StandardScaler: normalize data with zero mean and unit standar deviation\n",
    "* Normalizer: normalize each data point with unit l2 norm\n",
    "\n",
    "Finally we'll try reducing the dimensionality using topic modeling approach Latent Semantic Index (LSI, aka LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"labeledTrainData.tsv\",\n",
    "                    delimiter=\"\\t\",\n",
    "                    header=0,\n",
    "                    quoting=3)\n",
    "\n",
    "train_sents = pickle.load(open(\"train_sents.pickle\"))\n",
    "y = train[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize vectorizers as transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer=\"word\", stop_words=\"english\", max_features=200) \n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words=\"english\", max_features=200)\n",
    "hash_vectorizer = HashingVectorizer(analyzer='word', stop_words=\"english\", n_features=200)\n",
    "\n",
    "vectorizers = [count_vectorizer, tfidf_vectorizer, hash_vectorizer]\n",
    "standardizers = [StandardScaler(), Normalizer()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a simple linear model LogisticRegression to test out the quality of the vector representation in a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer = count, standardizer = standarscaler, accuracy = 0.788000\n",
      "\n",
      "vectorizer = count, standardizer = normalizer, accuracy = 0.788267\n",
      "\n",
      "vectorizer = tfidf, standardizer = standarscaler, accuracy = 0.788133\n",
      "\n",
      "vectorizer = tfidf, standardizer = normalizer, accuracy = 0.788133\n",
      "\n",
      "vectorizer = hash, standardizer = standarscaler, accuracy = 0.721600\n",
      "\n",
      "vectorizer = hash, standardizer = normalizer, accuracy = 0.722933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vectorizer, v_name in zip(vectorizers, [\"count\", \"tfidf\", \"hash\"]):\n",
    "    for standardizer, s_name in zip(standardizers, [\"standarscaler\", \"normalizer\"]):  \n",
    "        X = vectorizer.fit_transform(train_sents).toarray().astype(np.float64)\n",
    "        X = standardizer.fit_transform(X)\n",
    "        lr = LogisticRegression(random_state=42)\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        print \"vectorizer = %s, standardizer = %s, accuracy = %f\\n\" % (v_name, s_name, lr.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the size of the feature space from 200 to 2048:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer = count, standardizer = standarscaler, accuracy = 0.844667\n",
      "\n",
      "vectorizer = count, standardizer = normalizer, accuracy = 0.865333\n",
      "\n",
      "vectorizer = tfidf, standardizer = standarscaler, accuracy = 0.852400\n",
      "\n",
      "vectorizer = tfidf, standardizer = normalizer, accuracy = 0.871067\n",
      "\n",
      "vectorizer = hash, standardizer = standarscaler, accuracy = 0.721600\n",
      "\n",
      "vectorizer = hash, standardizer = normalizer, accuracy = 0.722933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizers[0].max_features = 2048\n",
    "vectorizers[1].max_features = 2048\n",
    "vectorizers[2].max_features = 2048\n",
    "\n",
    "for vectorizer, v_name in zip(vectorizers, [\"count\", \"tfidf\", \"hash\"]):\n",
    "    for standardizer, s_name in zip(standardizers, [\"standarscaler\", \"normalizer\"]): \n",
    "        X = vectorizer.fit_transform(train_sents).toarray().astype(np.float64)\n",
    "        X = standardizer.fit_transform(X)\n",
    "        lr = LogisticRegression(random_state=42)\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        print \"vectorizer = %s, standardizer = %s, accuracy = %f\\n\" % (v_name, s_name, lr.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that Tfidf has a slight edge over the other two vectorizers, and normatlizing vector with unit l2 norm always resulted in improved performance over the standard-scaler (0.0 mean, 1.0 std) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll try out the LSI method that represent documents as vectors in topic space. It's implemented as applying Singular Value Decomposition upon the bag of words representation of documents (with or without tfidf transformation). We could alternatively try the gensim implementation https://radimrehurek.com/gensim/models/lsimodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Topics = 32, accuracy = 0.830800\n",
      "Num of Topics = 64, accuracy = 0.844267\n",
      "Num of Topics = 128, accuracy = 0.862267\n",
      "Num of Topics = 256, accuracy = 0.872133\n",
      "Num of Topics = 512, accuracy = 0.873467\n",
      "Num of Topics = 1024, accuracy = 0.875600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_topics = [32, 64, 128, 256, 512, 1024]\n",
    "lsi = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer='word', stop_words=\"english\", max_features=4096)),\n",
    "                    (\"svd\", TruncatedSVD(n_components=100)), \n",
    "                    (\"normalizer\", Normalizer())])\n",
    "\n",
    "for n in n_topics:\n",
    "    lsi.set_params(svd__n_components=n)\n",
    "    X = lsi.fit_transform(train_sents)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    lr.fit(X_train, y_train).score(X_test, y_test)\n",
    "    \n",
    "    print \"Num of Topics = %d, accuracy = %f\" % (n, lr.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The accuracy increases with the number of topics, which suggests that more topics make the vector representation more expressive (but at the cost of increased computation)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
